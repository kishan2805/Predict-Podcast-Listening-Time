{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J86F8Wt5OioX",
        "outputId": "382fa39b-49b2-467a-b772-4c67b4c3fc9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqeMAFqANF8B",
        "outputId": "e02f6585-d041-4a6b-d0c1-d4747bf5d240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (662635, 12)\n",
            "Test shape: (250000, 11)\n",
            "\\nTrain data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 662635 entries, 0 to 662634\n",
            "Data columns (total 12 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   id                           662635 non-null  int64  \n",
            " 1   Podcast_Name                 662635 non-null  object \n",
            " 2   Episode_Title                662635 non-null  object \n",
            " 3   Episode_Length_minutes       585556 non-null  float64\n",
            " 4   Genre                        662634 non-null  object \n",
            " 5   Host_Popularity_percentage   662634 non-null  float64\n",
            " 6   Publication_Day              662634 non-null  object \n",
            " 7   Publication_Time             662634 non-null  object \n",
            " 8   Guest_Popularity_percentage  533675 non-null  float64\n",
            " 9   Number_of_Ads                662633 non-null  float64\n",
            " 10  Episode_Sentiment            662634 non-null  object \n",
            " 11  Listening_Time_minutes       662634 non-null  float64\n",
            "dtypes: float64(5), int64(1), object(6)\n",
            "memory usage: 60.7+ MB\n",
            "None\n",
            "\\nTrain data description:\n",
            "                  id  Episode_Length_minutes  Host_Popularity_percentage  \\\n",
            "count  662635.000000           585556.000000               662634.000000   \n",
            "mean   331317.000000               64.519379                   59.854599   \n",
            "std    191286.392149               32.964071                   22.869770   \n",
            "min         0.000000                0.000000                    1.300000   \n",
            "25%    165658.500000               35.770000                   39.410000   \n",
            "50%    331317.000000               63.870000                   60.050000   \n",
            "75%    496975.500000               94.070000                   79.510000   \n",
            "max    662634.000000              325.240000                  119.460000   \n",
            "\n",
            "       Guest_Popularity_percentage  Number_of_Ads  Listening_Time_minutes  \n",
            "count                533675.000000  662633.000000           662634.000000  \n",
            "mean                     52.239228       1.348388               45.440239  \n",
            "std                      28.451826       1.142717               27.135114  \n",
            "min                       0.000000       0.000000                0.000000  \n",
            "25%                      28.380000       0.000000               23.180910  \n",
            "50%                      53.590000       1.000000               43.393290  \n",
            "75%                      76.610000       2.000000               64.812760  \n",
            "max                     119.910000     103.910000              119.970000  \n",
            "\\nMissing values in train data:\n",
            "id                                  0\n",
            "Podcast_Name                        0\n",
            "Episode_Title                       0\n",
            "Episode_Length_minutes          77079\n",
            "Genre                               1\n",
            "Host_Popularity_percentage          1\n",
            "Publication_Day                     1\n",
            "Publication_Time                    1\n",
            "Guest_Popularity_percentage    128960\n",
            "Number_of_Ads                       2\n",
            "Episode_Sentiment                   1\n",
            "Listening_Time_minutes              1\n",
            "dtype: int64\n",
            "\\nNumeric features: 4\n",
            "Categorical features: 6\n",
            "\\nModel Evaluation:\n",
            "Ridge: RMSE = nan (±nan)\n",
            "Lasso: RMSE = nan (±nan)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "print(\"\\\\nTrain data info:\")\n",
        "print(train.info())\n",
        "\n",
        "print(\"\\\\nTrain data description:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"\\\\nMissing values in train data:\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "# Check the target variable distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train['Listening_Time_minutes'], kde=True)\n",
        "plt.title('Distribution of Listening Time')\n",
        "plt.savefig('listening_time_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Feature Engineering\n",
        "\n",
        "# Combine train and test for preprocessing\n",
        "target = 'Listening_Time_minutes'\n",
        "train_id = train['id']\n",
        "test_id = test['id']\n",
        "y_train = train[target]\n",
        "\n",
        "# Drop id and target from train\n",
        "train = train.drop(['id', target], axis=1)\n",
        "test = test.drop(['id'], axis=1)\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\\\nNumeric features: {len(numeric_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "# Create new features\n",
        "def create_features(df):\n",
        "    # Example feature engineering (you would customize this based on the actual data)\n",
        "    # For podcast data, we might create features like:\n",
        "\n",
        "    # 1. Extract time-related features if timestamps are available\n",
        "    if 'Release_Date' in df.columns:\n",
        "        df['Release_Date'] = pd.to_datetime(df['Release_Date'])\n",
        "        df['Release_Year'] = df['Release_Date'].dt.year\n",
        "        df['Release_Month'] = df['Release_Date'].dt.month\n",
        "        df['Release_Day'] = df['Release_Date'].dt.day\n",
        "        df['Release_DayOfWeek'] = df['Release_Date'].dt.dayofweek\n",
        "\n",
        "    # 2. Text length features if text descriptions are available\n",
        "    for col in categorical_features:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[f'{col}_Length'] = df[col].astype(str).apply(len)\n",
        "\n",
        "    # 3. Interaction features between numeric columns\n",
        "    for i, col1 in enumerate(numeric_features):\n",
        "        for col2 in numeric_features[i+1:]:\n",
        "            df[f'{col1}_times_{col2}'] = df[col1] * df[col2]\n",
        "            df[f'{col1}_plus_{col2}'] = df[col1] + df[col2]\n",
        "            df[f'{col1}_minus_{col2}'] = df[col1] - df[col2]\n",
        "            # Avoid division by zero\n",
        "            df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-5)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "train = create_features(train)\n",
        "test = create_features(test)\n",
        "\n",
        "# Update feature lists after feature engineering\n",
        "numeric_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Model Selection and Training\n",
        "\n",
        "# Function to evaluate models with cross-validation\n",
        "def evaluate_model(model, X, y, cv=5):\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_root_mean_squared_error')\n",
        "    return scores.mean(), scores.std()\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = train.copy()\n",
        "y = y_train.copy()\n",
        "\n",
        "# Define models to try\n",
        "models = {\n",
        "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso': Lasso(alpha=0.001, random_state=42),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate each model\n",
        "print(\"\\\\nModel Evaluation:\")\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    rmse_mean, rmse_std = evaluate_model(pipeline, X, y)\n",
        "    results[name] = rmse_mean\n",
        "    print(f\"{name}: RMSE = {rmse_mean:.4f} (±{rmse_std:.4f})\")\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = min(results, key=results.get)\n",
        "print(f\"\\\\nBest model: {best_model_name} with RMSE = {results[best_model_name]:.4f}\")\n",
        "\n",
        "# Hyperparameter tuning with Optuna for the best model\n",
        "def objective(trial):\n",
        "    if best_model_name == 'Ridge':\n",
        "        model = Ridge(\n",
        "            alpha=trial.suggest_float('alpha', 0.01, 10.0, log=True),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif best_model_name == 'Lasso':\n",
        "        model = Lasso(\n",
        "            alpha=trial.suggest_float('alpha', 0.0001, 1.0, log=True),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif best_model_name == 'RandomForest':\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
        "            max_depth=trial.suggest_int('max_depth', 3, 15),\n",
        "            min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif best_model_name == 'GradientBoosting':\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
        "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "            min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif best_model_name == 'XGBoost':\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
        "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "            subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
        "            random_state=42\n",
        "        )\n",
        "    else:  # LightGBM\n",
        "        model = lgb.LGBMRegressor(\n",
        "            n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
        "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "            num_leaves=trial.suggest_int('num_leaves', 20, 100),\n",
        "            subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    rmse_mean, _ = evaluate_model(pipeline, X, y)\n",
        "    return rmse_mean\n",
        "\n",
        "print(\"\\\\nHyperparameter tuning for the best model...\")\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(f\"Best trial: RMSE = {study.best_value:.4f}\")\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "if best_model_name == 'Ridge':\n",
        "    final_model = Ridge(**study.best_params, random_state=42)\n",
        "elif best_model_name == 'Lasso':\n",
        "    final_model = Lasso(**study.best_params, random_state=42)\n",
        "elif best_model_name == 'RandomForest':\n",
        "    final_model = RandomForestRegressor(**study.best_params, random_state=42)\n",
        "elif best_model_name == 'GradientBoosting':\n",
        "    final_model = GradientBoostingRegressor(**study.best_params, random_state=42)\n",
        "elif best_model_name == 'XGBoost':\n",
        "    final_model = xgb.XGBRegressor(**study.best_params, random_state=42)\n",
        "else:  # LightGBM\n",
        "    final_model = lgb.LGBMRegressor(**study.best_params, random_state=42)\n",
        "\n",
        "# Create the final pipeline\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', final_model)\n",
        "])\n",
        "\n",
        "# Train on the full training data\n",
        "final_pipeline.fit(X, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = final_pipeline.predict(test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Listening_Time_minutes': test_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\\\nSubmission file created!\")\n",
        "\n",
        "# Feature importance (if applicable)\n",
        "if hasattr(final_model, 'feature_importances_'):\n",
        "    # Get feature names after preprocessing\n",
        "    feature_names = []\n",
        "    for name, transformer, features in preprocessor.transformers_:\n",
        "        if name == 'num':\n",
        "            feature_names.extend(features)\n",
        "        elif name == 'cat':\n",
        "            for feature in features:\n",
        "                # Get one-hot encoded feature names\n",
        "                categories = transformer.named_steps['onehot'].categories_[features.index(feature)]\n",
        "                feature_names.extend([f\"{feature}_{category}\" for category in categories])\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = final_model.feature_importances_\n",
        "\n",
        "    # Sort feature importances\n",
        "    if len(importances) == len(feature_names):\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        top_features = [(feature_names[i], importances[i]) for i in indices[:20]]\n",
        "\n",
        "        print(\"\\\\nTop 20 important features:\")\n",
        "        for feature, importance in top_features:\n",
        "            print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "        # Plot feature importances\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.title('Feature Importances')\n",
        "        plt.barh(range(len(top_features)), [imp for _, imp in top_features], align='center')\n",
        "        plt.yticks(range(len(top_features)), [feat for feat, _ in top_features])\n",
        "        plt.xlabel('Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importances.png')\n",
        "        plt.close()\n",
        "\n",
        "# Stacking multiple models for better performance\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
        "    ('xgb', xgb.XGBRegressor(n_estimators=100, random_state=42)),\n",
        "    ('lgb', lgb.LGBMRegressor(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Create stacking regressor\n",
        "stacking_regressor = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LinearRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Create stacking pipeline\n",
        "stacking_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('stacking', stacking_regressor)\n",
        "])\n",
        "\n",
        "# Evaluate stacking model\n",
        "stacking_rmse, stacking_std = evaluate_model(stacking_pipeline, X, y)\n",
        "print(f\"\\\\nStacking Model: RMSE = {stacking_rmse:.4f} (±{stacking_std:.4f})\")\n",
        "\n",
        "# If stacking is better, use it as the final model\n",
        "if stacking_rmse < results[best_model_name]:\n",
        "    print(\"Stacking model is better than the best single model!\")\n",
        "\n",
        "    # Train stacking model on full data\n",
        "    stacking_pipeline.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    stacking_predictions = stacking_pipeline.predict(test)\n",
        "\n",
        "    # Create submission file\n",
        "    stacking_submission = pd.DataFrame({\n",
        "        'id': test_id,\n",
        "        'Listening_Time_minutes': stacking_predictions\n",
        "    })\n",
        "\n",
        "    stacking_submission.to_csv('stacking_submission.csv', index=False)\n",
        "    print(\"Stacking submission file created!\")\n",
        "\n",
        "print(\"\\\\nModel training and prediction completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxSuxhVuOKOI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}